{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97dd1dee",
   "metadata": {},
   "source": [
    "# **<u>Part 3</u>:** The best of two worlds: fast and readable Python.\n",
    "\n",
    "### \n",
    "\n",
    "### A GitHub analysis shows that `Python` is among the most expressive\n",
    "### general purpose programming languages in existence.\n",
    "### \n",
    "### Expressiveness means fewer lines of code and fewer changes per commit.\n",
    "### \n",
    "### Expressiveness comes at a cost: <u>performance</u>.\n",
    "### \n",
    "### Python is <u>slow</u>. Very slow. A pure `Python` implementation is typically\n",
    "### $\\sim 10$ to $\\sim 100$ times slower than an equivalent `C` implementation, for instance.\n",
    "### \n",
    "### Then why is `Python` arguably the most important language in scientific computing,\n",
    "### a domain wherein performance matters the most ?\n",
    "\n",
    "### \n",
    "### \n",
    "### \n",
    "### \n",
    "### 1. It is a misconception that performance matters a lot in scientific computing.\n",
    "### Usually, readability (`Python` is closest to `pseudocode`) > performance (debatable).\n",
    "### \n",
    "### 2. The `Python` programming language is one of the best suited languages for interfacing\n",
    "### performant languages (`Cython`, `Fortran`, `C`, `C++`, `Rust`, ...).\n",
    "### This has lead to a very rich ecosystem of quite performant scientific computing libraries\n",
    "### (`Numpy`, `Scipy`, `PyTorch`, ...) that combine the readability and ease of use of `Python`\n",
    "### with the performance of a compiled language (usually `C` or `Fortran`).\n",
    "### \n",
    "### Some say that `Python` is the _glue_ that holds all other languages together.\n",
    "### \n",
    "### This lecture teaches us the use of `Python` built-in and external modules that can speed up\n",
    "### our code considerably, without sacrificing too much readability.\n",
    "### \n",
    "### Before we venture on, here some artwork:\n",
    "### \n",
    "<img src=\"img/tree_final.png\" alt=\"Drawing\" width=\"700\"/>\n",
    "\n",
    "### \n",
    "\n",
    "### \n",
    "### <u>My code is slow. What should I do</u> ?!?\n",
    "### \n",
    "### Here, a very subjective hierarchy of steps you can take:\n",
    "### \n",
    "### 1. Have a beer (arguably the most comfortable solution).\n",
    "### 2. Improve your `Numpy` skills or write a parallelised code in `Python`.\n",
    "### 3. Use `JIT`-compiled libraries like `Numba` (general purpose) or `JAX` (anything related to optimisation).\n",
    "### 4. Learn the creole language `Cython`.\n",
    "### 5. Use the `ctypes` built-in module to interface a `C` code.\n",
    "### 6. Interface `C++`, `Rust`, `Fortran`, `...`\n",
    "### \n",
    "### If you find you need `C++` instead of `C / Cython / Numba`, you might soon reach a point \n",
    "### where it makes sense to rewrite everything in `C++`, and the \n",
    "### `Python` interface becomes a mere afterthought.\n",
    "### \n",
    "### <u>Remember</u>: \n",
    "### Each optimisation may sacrifice readability / flexibility. \n",
    "### Use it with care. Often you will find, you actually don't really need it.\n",
    "### \n",
    "### _\"Premature optimization is the root of all evil.\"_\n",
    "### - Donald Knuth\n",
    "### \n",
    "### _\"Python where we can, C where we must.\"_\n",
    "### - Sergey Brin (Google)\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "# <u>Lesson 1</u>:\n",
    "## Advanced `Numpy` concepts.\n",
    "### \n",
    "### Once you grow tired of the _'have a beer'_ solution, the next step you should take is pimping your use of `Numpy`.\n",
    "### <u>Here is why</u>: making good use of `Numpy` does not only speed up your code, it also typically makes it more `readable`.\n",
    "### \n",
    "### Check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be460e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Sequence\n",
    "\n",
    "\"\"\"\n",
    "  Four different functions that operate on an array\n",
    "  of shape (n0, n1, ..., nM-1, nM) and return an array of shape\n",
    "  (n0, n1, ..., nM-1) that takes the l2 norm along the last axis.\n",
    "  \n",
    "  The shape of the array (number of dimensions) is only known at runtime.\n",
    "\"\"\"\n",
    "\n",
    "### BAD implementations first\n",
    "\n",
    "# the non-idiomatic implementation requires a number of fail checks to work exactly right\n",
    "def compute_l2_norm_along_last_axis_very_bad(arr: np.ndarray | Sequence) -> np.ndarray:\n",
    "  # remember, you have no idea what the shape of the array is\n",
    "  arr = np.asarray(arr)  # do local coercion\n",
    "  assert (y := arr.ndim) >= 1, f\"Expected the array to have at least one dimension.\"\n",
    "  shape = arr.shape  # (n0, n1, .., nM)\n",
    "  \n",
    "  # Since we don't know the number of dimensions,\n",
    "  # we have to flatten the first couple of axes \n",
    "  # believe me, a lot of people do this ...\n",
    "  arr = arr.reshape((np.prod(shape[:-1]), shape[-1]))\n",
    "  \n",
    "  ret = np.zeros(arr.shape[:-1], dtype=float)\n",
    "  \n",
    "  for i in range(len(ret)):\n",
    "    val = 0.0\n",
    "    for j in range(len(arr[i])):\n",
    "      val += arr[i, j] ** 2\n",
    "      \n",
    "    ret[i] = val ** .5\n",
    "  \n",
    "  return ret.reshape(shape[:-1])  # reshape \n",
    "\n",
    "\n",
    "def compute_l2_norm_along_last_axis_bad(arr: np.ndarray | Sequence) -> np.ndarray:\n",
    "  # a bit better\n",
    "  arr = np.asarray(arr)\n",
    "  shape = arr.shape\n",
    "  \n",
    "  arr = arr.reshape((-1, shape[-1]))  # -1 infers the remaining dimension\n",
    "  \n",
    "  ret = np.empty( arr.shape[:-1], dtype=float )  # allocating empty array is faster than zero array\n",
    "  \n",
    "  for i, subarr in enumerate(arr):  # iterate over array directly rather than using a range\n",
    "    ret[i] = (subarr ** 2).sum() ** .5  # avoid the second for loop using vectorisation\n",
    "    \n",
    "  return ret.reshape(shape[:-1])\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "### Now the `good` ones\n",
    "\n",
    "\n",
    "# We don't need fail checks. Will fail if `arr` doesn't have the right shape.\n",
    "def compute_l2_norm_along_last_axis_good(arr: np.ndarray) -> np.ndarray:\n",
    "  return np.sqrt((np.asarray(arr) ** 2).sum(-1))\n",
    "\n",
    "\n",
    "# idem\n",
    "compute_l2_norm_along_last_axis_idiomatic = lambda arr: np.linalg.norm(arr, ord=2, axis=-1)\n",
    "\n",
    "\n",
    "### \n",
    "  \n",
    "  \n",
    "# an array with some random shape\n",
    "A = np.random.randn(5, 4, 2, 3, 20)\n",
    "\n",
    "norm0 = compute_l2_norm_along_last_axis_idiomatic(A)\n",
    "\n",
    "### check if all implementations give the same outcome\n",
    "allfuncs = (compute_l2_norm_along_last_axis_very_bad, \n",
    "            compute_l2_norm_along_last_axis_bad,\n",
    "            compute_l2_norm_along_last_axis_good,\n",
    "            compute_l2_norm_along_last_axis_idiomatic)\n",
    "\n",
    "print(f\"All implementations are the same: {all(np.allclose(norm0, f(A)) for f in allfuncs)}\")\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03be59",
   "metadata": {},
   "source": [
    "### Let's see which one is the fastest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa869f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit compute_l2_norm_along_last_axis_very_bad(A)\n",
    "%timeit compute_l2_norm_along_last_axis_bad(A)\n",
    "%timeit compute_l2_norm_along_last_axis_good(A)\n",
    "%timeit compute_l2_norm_along_last_axis_idiomatic(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03775e42",
   "metadata": {},
   "source": [
    "### \n",
    "### We have two clear winners in terms of readability and speed !\n",
    "### \n",
    "### When you use a `numpy` function like `np.linalg.norm`, there is usually\n",
    "### an `axis` argument which you can use to perform an operation along an axis.\n",
    "### \n",
    "### `Numpy` programming $\\neq$ `Python` programming.\n",
    "### If you know `Python` you don't automatically know how to use `Numpy`.\n",
    "### However, if you're really good at `Numpy` you're typically also really good at `Python`.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### We come back to our `derivative.py` code from the `OOP` lecture.\n",
    "### It's exactly the same code but now I allowed everything to be a vectorised\n",
    "### `Numpy` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c3eb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# %load vector/derivative_vectorised.py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Sequence, Tuple, Union, Any, Callable\n",
    "from abc import abstractmethod\n",
    "from collections.abc import Hashable\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "NumericType = Union[int, float]\n",
    "FunctionType = Union['DifferentiableFunction', NumericType]\n",
    "CallInputType = Union[NumericType, np.ndarray, Sequence[NumericType]]\n",
    "\n",
    "\n",
    "# main function for type coercion\n",
    "def as_function(func: FunctionType) -> 'DifferentiableFunction':\n",
    "  \"\"\"\n",
    "    func is a DifferentialFunction => return func,\n",
    "    func is an int or float => return Constant(func),\n",
    "    func is anything else => this method fails.\n",
    "  \"\"\"\n",
    "  if isinstance(func, DifferentiableFunction):\n",
    "    return func\n",
    "  return Constant(func)\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def _derivative(self: 'DifferentiableFunction') -> 'DifferentiableFunction':\n",
    "  \"\"\"\n",
    "  All our classes are hashable. They can be used in combination with lru_cache.\n",
    "  We call this function in DifferentiableFunction.derivative().\n",
    "  We will never again compute a derivative twice ;-)\n",
    "  \"\"\"\n",
    "  return self._deriv()\n",
    "\n",
    "\n",
    "class DifferentiableFunction(Hashable):\n",
    "\n",
    "  def __init__(self, args: Sequence[Hashable]) -> None:\n",
    "    self._args = tuple(args)\n",
    "    self._hash = hash(self._args)\n",
    "\n",
    "  def __hash__(self) -> int:\n",
    "    return self._hash\n",
    "\n",
    "  def __eq__(self, other: Any) -> bool:\n",
    "    return self.__class__ == other.__class__ and hash(self) == hash(other) and self._args == other._args\n",
    "\n",
    "  @abstractmethod\n",
    "  def __call__(self, x: np.ndarray | NumericType | Sequence[NumericType]):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def _deriv(self):\n",
    "    pass\n",
    "\n",
    "  def derivative(self, n: int = 1) -> 'DifferentiableFunction':\n",
    "    assert (n := int(n)) >= 0\n",
    "    if n == 0:\n",
    "      return self\n",
    "    # use the cached version.\n",
    "    return _derivative(self).derivative(n=n-1)\n",
    "\n",
    "  def plot(self, interval: Tuple[int, int] = (0, 1), npoints: int = 1001) -> None:\n",
    "    \"\"\" Plot function over the interval `interval` using `npoints` function evaluations. \"\"\"\n",
    "    a, b = interval\n",
    "    assert b > a\n",
    "    x = np.linspace(*interval, 1001)\n",
    "    y = self(x)\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "\n",
    "  def __add__(self, other: FunctionType) -> 'Add':\n",
    "    \"self: DifferentialFunction + other: FunctionType\"\n",
    "    return Add(self, other)\n",
    "\n",
    "  __radd__ = __add__\n",
    "\n",
    "  def __mul__(self, other: FunctionType) -> 'Multiply':\n",
    "    \"self: DifferentialFunction * other: FunctionType\"\n",
    "    return Multiply(self, other)\n",
    "\n",
    "  __rmul__ = __mul__\n",
    "\n",
    "  def __sub__(self, other: FunctionType) -> 'Add':\n",
    "    \"self: DifferentialFunction - other: FunctionType\"\n",
    "    return self + (-1) * other\n",
    "\n",
    "  def __rsub__(self, other: NumericType) -> 'Add':\n",
    "    \"\"\"\n",
    "      other: NumericType - self: DifferentialFunction.\n",
    "      Here, the -1 has to go in front of self.\n",
    "      other - self => self.__rsub__(other).\n",
    "    \"\"\"\n",
    "    return other + (-1) * self\n",
    "\n",
    "\n",
    "class Constant(DifferentiableFunction):\n",
    "\n",
    "  def _deriv(self) -> 'Constant':\n",
    "    return Constant(0)\n",
    "\n",
    "  def __init__(self, value: NumericType) -> None:\n",
    "    self.value = float(value)\n",
    "    super().__init__([self.value])\n",
    "\n",
    "  def __call__(self, x: CallInputType) -> float:\n",
    "    return self.value\n",
    "\n",
    "\n",
    "class Argument(DifferentiableFunction):\n",
    "\n",
    "  def _deriv(self) -> Constant:\n",
    "    return Constant(1)\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__([])\n",
    "\n",
    "  def __call__(self, x: CallInputType) -> np.ndarray:\n",
    "    return np.asarray(x, dtype=float)\n",
    "\n",
    "\n",
    "class Add(DifferentiableFunction):\n",
    "\n",
    "  def _deriv(self) -> 'Add':\n",
    "    return self.f0.derivative() + self.f1.derivative()\n",
    "\n",
    "  def __init__(self, f0: FunctionType, f1: FunctionType) -> None:\n",
    "    self.f0, self.f1 = sorted(map(as_function, (f0, f1)), key=lambda x: (x.__class__.__name__, hash(x)))\n",
    "    super().__init__([self.f0, self.f1])\n",
    "\n",
    "  def __call__(self, x: CallInputType) -> np.ndarray:\n",
    "    return self.f0(x) + self.f1(x)\n",
    "\n",
    "\n",
    "class Multiply(DifferentiableFunction):\n",
    "\n",
    "  def _deriv(self) -> Add:\n",
    "    return self.f0.derivative() * self.f1 + self.f0 * self.f1.derivative()\n",
    "\n",
    "  def __init__(self, f0: FunctionType, f1: FunctionType) -> None:\n",
    "    self.f0, self.f1 = sorted(map(as_function, (f0, f1)), key=lambda x: (x.__class__.__name__, hash(x)))\n",
    "    super().__init__([self.f0, self.f1])\n",
    "\n",
    "  def __call__(self, x: CallInputType) -> np.ndarray:\n",
    "    return self.f0(x) * self.f1(x)\n",
    "\n",
    "\n",
    "class ChainRule(DifferentiableFunction):\n",
    "\n",
    "  evalf: Callable\n",
    "  df: Callable\n",
    "\n",
    "  def _deriv(self) -> DifferentiableFunction:\n",
    "    return self.df(self.argument) * self.argument.derivative()\n",
    "\n",
    "  def __init__(self, argument: FunctionType) -> None:\n",
    "    assert all(hasattr(self, item) for item in ('evalf', 'df')), 'Each derived class needs to implement `evalf` and `df`.'\n",
    "    self.argument = as_function(argument)\n",
    "    super().__init__([self.argument])\n",
    "\n",
    "  def __call__(self, x: CallInputType) -> np.ndarray:\n",
    "    return self.evalf(self.argument(x))\n",
    "\n",
    "\n",
    "class Exp(ChainRule):\n",
    "  evalf = np.exp\n",
    "  df = lambda self, argument: self\n",
    "\n",
    "\n",
    "class Sin(ChainRule):\n",
    "  evalf = np.sin\n",
    "  df = lambda self, argument: Cos(argument)\n",
    "\n",
    "\n",
    "class Cos(ChainRule):\n",
    "  evalf = np.cos\n",
    "  df = lambda self, argument: (-1) * Sin(argument)\n",
    "\n",
    "\n",
    "# make an argument f(x) = x\n",
    "x = Argument()\n",
    "\n",
    "# choose some c0, c1\n",
    "c0, c1 = 2, 1\n",
    "\n",
    "# make the damping term\n",
    "exp = Exp(-.5 * x)\n",
    "\n",
    "# define the natural frequency\n",
    "w0 = 3 * np.sqrt(11) / 2\n",
    "\n",
    "# create y(x) using syntactic sugar\n",
    "y = c0 * exp * Sin(w0 * x) + c1 * exp * Cos(w0 * x)\n",
    "\n",
    "complex_func = 25 * y + y.derivative() + y.derivative(2) + y.derivative(3) + y.derivative(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e5106",
   "metadata": {},
   "source": [
    "### \n",
    "### We time the vectorised and the non-vectorised versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6980ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.linspace(0, 1, 1001)\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "complex_func(xi)\n",
    "t1 = time.time()\n",
    "print(f\"Vectorised operation took {t1 - t0} seconds.\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "[complex_func(_xi) for _xi in xi]\n",
    "t1 = time.time()\n",
    "print(f\"for loop took {t1 - t0} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5341b6",
   "metadata": {},
   "source": [
    "### \n",
    "### You don't really need custom compilation here. `Numpy` is fast enough for this task.\n",
    "\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "\n",
    "### In the following, a quick wrap-up of the basics of `numpy`. Since this is an _advanced_\n",
    "### `Python` course, I assume that everyone has some familiarity.\n",
    "### \n",
    "\n",
    "### The most important tool that `numpy` gives us is the `numpy.newaxis` variable which is\n",
    "### a convenient alias for `None`. For improved readability it is very common to set `_ = np.newaxis`. \n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = np.newaxis\n",
    "\n",
    "print(f\"_ == np.newaxis == None ? {_ == np.newaxis == None}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5455378",
   "metadata": {},
   "source": [
    "### \n",
    "### The `_` variable allows us to create a new `artificial` axis, which creates an axis of\n",
    "### dimension `1` at the spot where we put it.\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b962589",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(2, 3)\n",
    "\n",
    "print(f\"         A.shape:    {A.shape}.\\n\")\n",
    "print(f\"      A[_].shape: {A[_].shape}.\\n\")\n",
    "print(f\"   A[:, _].shape: {A[:, _].shape}.\\n\")\n",
    "print(f\"A[:, :, _].shape: {A[:, :, _].shape}.\\n\")\n",
    "print(f\" A[..., _].shape: {A[..., _].shape}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d8c8a",
   "metadata": {},
   "source": [
    "### \n",
    "### Here the `Ellipsis` variable `...` _infers_ all remaining dimensions.\n",
    "\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### When we perform an operation between two `np.ndarray`'s of shape\n",
    "### `(n0, n1, ..., nM)` and\n",
    "### `(m0, m1, ..., mM)`\n",
    "### the output shape is: \n",
    "### `(max(n0, m0), max(n1, m1), ..., max(nM, mM))`. \n",
    "### If `ni != mi` then either `ni == 1` or `mi == 1` must hold.\n",
    "### \n",
    "### Let's suppose for now that there are no `1` axes. The following code\n",
    "### roughly reproduces a vectorised operation using some `np.ufunc`\n",
    "### (but is a trillion times slower):\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def numpy_equivalent_ufunc_operation(arr0: np.ndarray, arr1: np.ndarray, ufunc: np.ufunc) -> np.ndarray:\n",
    "  assert arr0.shape == arr1.shape and 1 not in arr0.shape, NotImplementedError\n",
    "  # we are also assuming that the data type is `float` for convenience\n",
    "  ret = np.empty(arr0.shape, dtype=float)\n",
    "  \n",
    "  # iterate over all combinations (i0, i1, i2, ...) from our shape\n",
    "  for multi_index in product(*map(range, arr0.shape)):\n",
    "    ret[multi_index] = ufunc(arr0[multi_index], arr1[multi_index])\n",
    "    \n",
    "  return ret\n",
    "\n",
    "\n",
    "arr0 = np.random.randn(2, 3, 4)\n",
    "arr1 = np.random.randn(2, 3, 4)\n",
    "\n",
    "print(f\"np.add(arr0, arr1) is equal to our implementation ? {np.allclose(np.add(arr0, arr1), numpy_equivalent_ufunc_operation(arr0, arr1, np.add))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a2d22",
   "metadata": {},
   "source": [
    "### \n",
    "### What changes when we DO allow for `1` axes ?\n",
    "### For an implementation, see below:\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple\n",
    "from itertools import product\n",
    "\n",
    "_ = np.newaxis\n",
    "\n",
    "\n",
    "def truncate_multi_index(multi_index: Tuple[int, ...], shape: Tuple[int, ...]) -> Tuple[int, ...]:\n",
    "  \"\"\"\n",
    "    (1, 2, 1, 4, 5), (2, 3, 3, 6, 7) -> (1, 2, 1, 4, 5)\n",
    "    (1, 2, 1, 4, 5), (2, 3, 3, 1, 1) -> (1, 2, 1, 0, 0)\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    ETC\n",
    "  \"\"\"\n",
    "  return tuple( i if j != 1 else 0 for (i, j) in zip(multi_index, shape, strict=True) )\n",
    "\n",
    "\n",
    "def numpy_equivalent_ufunc_operation(arr0: np.ndarray, arr1: np.ndarray, ufunc: np.ufunc) -> np.ndarray:\n",
    "  assert all( i == j or 1 in (i, j) for i, j in zip(arr0.shape, arr1.shape, strict=True) )\n",
    "  \n",
    "  shape = tuple(map(max, zip(arr0.shape, arr1.shape)))  # output shape\n",
    "\n",
    "  ret = np.empty(shape, dtype=float)\n",
    "  \n",
    "  # iterate over all combinations (i0, i1, i2, ...) from our shape\n",
    "  for multi_index in product(*map(range, shape)):\n",
    "    ret[multi_index] = ufunc( \n",
    "                              arr0[ truncate_multi_index(multi_index, arr0.shape) ],\n",
    "                              arr1[ truncate_multi_index(multi_index, arr1.shape) ]\n",
    "                            )\n",
    "    \n",
    "  return ret\n",
    "\n",
    "\n",
    "arr0 = np.random.randn(2, 3, 4)[:, _]\n",
    "arr1 = np.random.randn(2, 3, 4)[:, :, _]\n",
    "\n",
    "print(f\"np.add(arr0, arr1) is equal to our implementation ? {np.allclose(np.add(arr0, arr1), numpy_equivalent_ufunc_operation(arr0, arr1, np.add))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066176b",
   "metadata": {},
   "source": [
    "### \n",
    "### Let `arr` of shape `(n0, ..., nN-1, nN, ..., nM)` be cast to \n",
    "###     `arr_` of shape `(n0, ..., nN-1, 1, nN, ..., nM)`.\n",
    "### \n",
    "### `Numpy` then broadcasts `arr_` such that\n",
    "### `arr_[i0, ..., iN-1, j, iN, ..., iM] == arr[i0, ..., iN-1, iN, ..., iM]`\n",
    "### for all `j` that are smaller than `pj` in the output shape `(..., pj, ...)`.\n",
    "### \n",
    "### When the dimension of the arrays we work with is small, it is helpful to visualize\n",
    "### things as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_ = np.newaxis\n",
    "\n",
    "arr0 = np.random.randn(3)\n",
    "arr1 = np.random.randn(5)\n",
    "\n",
    "outer_diff = arr0[:, _] - arr1[_, :]   # has shape (3, 5)\n",
    "\n",
    "print(f\"arr0[:, _] is broadcast to: \\n\\n{np.broadcast_to(arr0[:, _], outer_diff.shape)}.\\n\")\n",
    "print(f\"arr1[_, :] is broadcast to: \\n\\n{np.broadcast_to(arr1[_, :], outer_diff.shape)}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0ae88",
   "metadata": {},
   "source": [
    "### \n",
    "### When the number of dimensions is larger, it is better to think in terms of shapes.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "## <u> Exercise 1.1 </u>:\n",
    "### Given an array `arr0` of shape `(p, n)` and `arr1` of shape `(q, n)`, compute the\n",
    "### matrix `dist` of shape `(p, q)` containing all Euclidean distances between the\n",
    "### row vectors of `arr0` and `arr1`. You're not allowed to use `np.linalg.norm` ;-)\n",
    "### \n",
    "### `dist[i, j]` = `|| arr0[i, :] - arr1[j, :] ||`.\n",
    "### \n",
    "\n",
    "### **HINT**:\n",
    "### `(p, n)` and `(q, n)` become\n",
    "### `(p, 1, n)` and `(1, q, n)` become \n",
    "### `(p, q, n)` and `(p, q, n)` becomes\n",
    "### `(p, q)`.\n",
    "### \n",
    "### <u>solution</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0771cd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = np.newaxis\n",
    "\n",
    "p, q, n = 4, 6, 10\n",
    "\n",
    "arr0 = np.random.randn(p, n)\n",
    "arr1 = np.random.randn(q, n)\n",
    "\n",
    "### all equivalent\n",
    "dist0 = ((arr0[:, _, :] - arr1[_, :, :])**2).sum(-1) ** .5\n",
    "\n",
    "# We can leave out the trailing colons `:`. Numpy will infer them.\n",
    "dist1 = ((arr0[:, _] - arr1[_])**2).sum(-1) ** .5\n",
    "\n",
    "# np.sqrt(x) is a tad faster than x ** .5 because it's optimised for square root only.\n",
    "dist2 = np.sqrt( ((arr0[:, _] - arr1[_])**2).sum(-1) )\n",
    "\n",
    "print(f\"All are equivalent ? {np.allclose(dist0, dist1) and np.allclose(dist1, dist2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ea529",
   "metadata": {},
   "source": [
    "### \n",
    "### Thinking in terms of shapes is no longer optional when the dimension of the\n",
    "### broadcast arrays exceeds $3$ (unless you came to visit us in our _flatland_ from a higher-dimensional universe).\n",
    "### \n",
    "### Luckily, this trick works in **the vast majority of cases** (and is therefore what I recommend you shoud do).\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### \n",
    "### We continue with another (preparational)\n",
    "## <u> Exercise 1.2 </u>:\n",
    "### Given arrays `arr0, arr1` of shape `(p, n)` and `(n, q)`, implement matrix multiplication\n",
    "### using vectorisation (don't use `@` or `np.matmul`).\n",
    "### \n",
    "### <u>solution</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a95e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_ = np.newaxis\n",
    "\n",
    "p, q, n = 4, 6, 10\n",
    "\n",
    "arr0 = np.random.randn(p, n)\n",
    "arr1 = np.random.randn(n, q)\n",
    "\n",
    "# (p, n) and (n, q) becomes\n",
    "# (p, n, 1) and (1, n, q) becomes\n",
    "# (p, n, q) and (p, n, q) becomes\n",
    "# (p, q)\n",
    "\n",
    "### two equivalent ones\n",
    "matmul0 = (arr0[:, :, _] * arr1[_, :, :]).sum(1)\n",
    "\n",
    "matmul1 = (arr0[..., _] * arr1).sum(1)\n",
    "\n",
    "matmul = arr0 @ arr1\n",
    "print(f\"matmul0 and matmul1 are both equal to matmul0 @ matmul1 ? {np.allclose(matmul0, matmul) and np.allclose(matmul1, matmul)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250d459",
   "metadata": {},
   "source": [
    "### \n",
    "### \n",
    "### \n",
    "### \n",
    "### \n",
    "### The big question is: why were we allowed to do `(arr0[..., _] * arr1)` ?\n",
    "### `arr0[..., _]` has three dimensions `(p, n, 1)` while `arr1` has two `(n, q)` ?!?\n",
    "### \n",
    "### $\\implies$ If one array is _shorter_ than the other, `Numpy` prepends as many `1` axes as necessary !\n",
    "### `(p, n, 1)` and `(n, q)` becomes `(p, n, 1)` and `(1, n, q)`.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### Now suppose you have an array of dimension `N` and you would like to add an artificial axis at the\n",
    "### `m`-th spot, with `m` < `N`. However you know `m` only at runtime. What do you do ?\n",
    "### \n",
    "### You have to somehow pass `m` colons `:` into the array brackets and then a new axis.\n",
    "### For instance, `arr.shape == (3, 4, 5, 6, 7, 2, 2)` and `m == 3`\n",
    "### $\\implies$ `arr[:, :, :, _].shape == (3, 4, 5, 1, 6, 7, 2, 2)`.\n",
    "### \n",
    "### Here is how you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3424808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = np.newaxis\n",
    "\n",
    "# this guy is equivalent to a colon `:`\n",
    "sl = slice(None)\n",
    "\n",
    "\n",
    "def add_axis_m(arr: np.ndarray, m: int) -> np.ndarray:\n",
    "  # m colons plus a new axis =)\n",
    "  return arr[ (sl,) * m + (_,) ]\n",
    "\n",
    "\n",
    "A = np.random.randn(3, 4, 5, 6, 7, 2, 2)\n",
    "\n",
    "print(f\"A has shape: {A.shape}.\\n\")\n",
    "print(f\"Adding new axis to `A` at the 3rd spot gives shape: {add_axis_m(A, 3).shape}. \\n\")\n",
    "print(f\"Adding new axis to `A` at the 5th spot gives shape: {add_axis_m(A, 5).shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb35ca",
   "metadata": {},
   "source": [
    "### \n",
    "### Actually not that difficult ;-)\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### Now that we know how to multiply matrices using vectorisation and how to add axes in specific spots,\n",
    "### we will challenge ourselves a bit more.\n",
    "### \n",
    "## <u>Exercise 1.3</u>:\n",
    "### Given two arrays `arr0` and `arr1`, both of unknown dimension `N`.\n",
    "### Compute an array `matmul_m` that performs `matrix multiplication` along axes numbers `m` and `m+1`, with `m+1 < N`.\n",
    "### \n",
    "### I.e., `matmul_m` satisfies: \n",
    "### `//   matmul_m[i0, ..., i_(m-1),| :, :,| i_(m+2), ..., iN]`\n",
    "### `// =     arr0[    ...          | :, :,|          ...    ]`\n",
    "### `//     @ arr1[    ...          | :, :,|          ...    ]`.\n",
    "### This problem is solved by **thinking in terms of shapes !!**\n",
    "### \n",
    "### \n",
    "### <u>HINT</u>: here is what needs to happen (focus on the part between the `|   |`):\n",
    "### `(n0, ..., n_(m-1),| p, n,    | n_(m+2), ..., nN)` and \n",
    "### `(n0, ..., n_(m-1),|    n, q, | n_(m+2), ..., nN)` \n",
    "### becomes\n",
    "### `(    ...          | p, n, 1, |          ...    )` and \n",
    "### `(    ...          | 1, n, q, |          ...    )` \n",
    "### becomes\n",
    "### `(    ...          | p, n, q, |          ...    )` and \n",
    "### `(    ...          | p, n, q, |          ...    )` \n",
    "### becomes\n",
    "### `(n0, ..., n_(m-1),| p,    q, | n_(m+2), ..., nN)`.\n",
    "### \n",
    "### Compared to our previous implementation, there is a `head` and a `tail` we have to carry around.\n",
    "### \n",
    "### <u>Template</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = np.newaxis\n",
    "sl = slice(None)\n",
    "\n",
    "\n",
    "def perform_matmul_m(arr0: np.ndarray, arr1: np.ndarray, m: int) -> np.ndarray:\n",
    "  assert arr0.ndim == arr1.ndim and m + 1 < arr0.ndim\n",
    "  return # ??? YOUR ONE-LINER HERE\n",
    "\n",
    "\n",
    "m = 3\n",
    "arr0 = np.random.randn(3, 4, 5, 6, 7, 2, 2)\n",
    "arr1 = np.random.randn(3, 4, 5, 7, 3, 2, 2)\n",
    "matmul_3 = perform_matmul_m(arr0, arr1, 3)\n",
    "\n",
    "print(f\"                arr0 has shape: {arr0.shape}.\\n\")\n",
    "print(f\"                arr1 has shape: {arr1.shape}.\\n\")\n",
    "print(f\"matmul_m with m == 3 has shape: {matmul_3.shape}. \\n\\n\")\n",
    "\n",
    "multi_index = (0, 0, 0, sl, sl, 0, 0)\n",
    "print(f\"multi_index equals: {multi_index}.\\n\")\n",
    "\n",
    "print(f\"matmul_3[multi_index] equals arr0[multi_index] @ arr1[multi_index] ? {np.allclose(matmul_3[multi_index], arr0[multi_index] @ arr1[multi_index])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdef6e7",
   "metadata": {},
   "source": [
    "### <u>solution</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b71cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = np.newaxis\n",
    "sl = slice(None)\n",
    "\n",
    "\n",
    "def perform_matmul_m(arr0: np.ndarray, arr1: np.ndarray, m: int) -> np.ndarray:\n",
    "  assert arr0.ndim == arr1.ndim and m + 1 < arr0.ndim\n",
    "  # arr0[ (m + 2) colons and a new axis ]\n",
    "  # arr1[  m      colons and a new axis ]\n",
    "  return (arr0[ (sl,) * (m+2) + (_,)  ] * arr1[ (sl,) * m + (_,) ]).sum(m+1)\n",
    "\n",
    "\n",
    "m = 3\n",
    "arr0 = np.random.randn(3, 4, 5, 6, 7, 2, 2)\n",
    "arr1 = np.random.randn(3, 4, 5, 7, 3, 2, 2)\n",
    "matmul_3 = perform_matmul_m(arr0, arr1, 3)\n",
    "\n",
    "print(f\"                arr0 has shape: {arr0.shape}.\\n\")\n",
    "print(f\"                arr1 has shape: {arr1.shape}.\\n\")\n",
    "print(f\"matmul_m with m == 3 has shape: {matmul_3.shape}. \\n\\n\")\n",
    "\n",
    "multi_index = (0, 0, 0, sl, sl, 0, 0)\n",
    "print(f\"multi_index equals: {multi_index}.\\n\")\n",
    "\n",
    "print(f\"matmul_3[multi_index] equals arr0[multi_index] @ arr1[multi_index] ? {np.allclose(matmul_3[multi_index], arr0[multi_index] @ arr1[multi_index])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bc5a2",
   "metadata": {},
   "source": [
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### It is worth it to understand `Numpy` better. You'll be surprised by how much stuff\n",
    "### you can solve by making better use of `Numpy`.\n",
    "### \n",
    "## <u> Task </u>:\n",
    "### Given an array of arbitrary shape `(n0, ..., nM)`, write a function `normalize(arr)`\n",
    "### that normalizes `arr` along the last axis `in place` !\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_ = np.newaxis\n",
    "\n",
    "\n",
    "def normalize_bad(arr: np.ndarray) -> None:\n",
    "  \n",
    "  # ( n0,  n1,  ...,  n_(N-1),   nN) becomes\n",
    "  # ( n0 x n1 x ... x n_(N-1),   nN)\n",
    "  arr = arr.reshape( (-1, arr.shape[-1]) )  # view into the same memory\n",
    "  \n",
    "  for i, subarr in enumerate(arr):\n",
    "    arr[i] /= np.linalg.norm(subarr, ord=2)\n",
    "    \n",
    "    \n",
    "def normalize_ok(arr: np.ndarray) -> None:\n",
    "  norm = np.linalg.norm(arr, ord=2, axis=-1)\n",
    "  arr[:] = arr / norm[..., _]\n",
    "  # here it would be better to say arr /= norm[..., _]\n",
    "\n",
    "\n",
    "def normalize_good(arr: np.ndarray) -> None:\n",
    "  np.divide(arr, np.linalg.norm(arr, ord=2, axis=-1, keepdims=True), out=arr)\n",
    "  \n",
    "  \n",
    "  \n",
    "A = np.random.randn(4, 3, 2, 6, 10)\n",
    "\n",
    "A0 = A.copy()\n",
    "normalize_bad(A0)\n",
    "\n",
    "A1 = A.copy()\n",
    "normalize_ok(A1)\n",
    "\n",
    "A2 = A.copy()\n",
    "normalize_good(A2)\n",
    "\n",
    "print(f'All three implementations give the same value: {all(np.allclose(A0, Ai) for Ai in (A1, A2))}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit normalize_bad(A)\n",
    "%timeit normalize_ok(A)\n",
    "%timeit normalize_good(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7181b",
   "metadata": {},
   "source": [
    "### \n",
    "### The last two implementations are an order of magnitude faster because they avoid the for loop.\n",
    "### \n",
    "### The third implementation is a tad faster than the second because:\n",
    "### 1. The line `arr[:] = arr / norm[..., _]` actually creates a new array `y := arr / norm[..., _]`\n",
    "### which is then copied back into `arr`.\n",
    "### 2. The broadcast `norm[..., _]` creates a slight overhead.\n",
    "### \n",
    "### In the last implementation, we avoided the broadcast by passing `keepdims=True` in the `np.linalg.norm`\n",
    "### function. Each function that performs an axis reduction has this keyword argument. It works as follows:\n",
    "### \n",
    "### `reduction_func(arr, axis=(2, 4, 5))`:\n",
    "### shape `(2, 3, 4, 2, 5, 6, 6)` becomes `(2, 3, 2, 6)`\n",
    "### \n",
    "### `reduction_func(arr, axis=(2, 4, 5), keepdims=True):`\n",
    "### shape `(2, 3, 4, 2, 5, 6, 6)` becomes `(2, 3, 1, 2, 1, 1, 6)`.\n",
    "\n",
    "### \n",
    "### The `keepdims=True` keyword is very helpful for vectorisation.\n",
    "### \n",
    "\n",
    "### More importantly: in the `np.divide` function we passed `out=arr` which avoids allocating new memory\n",
    "### but performs the operation into the existing array `arr` we passed.\n",
    "### \n",
    "\n",
    "### It is worth studying the the extra arguments of `np.ufunc`'s. \n",
    "### If you're struggling with something, you're probably not the first person to do so.\n",
    "### The guys at `numpy` try to address these issues and continuously add functionality that people need.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "\n",
    "### As a last topic on `Numpy`, we discuss `masked` arrays. Unless you work with stochastic processes\n",
    "### or statistical data, you won't need them often but it's nevertheless a nice gimmick.\n",
    "### \n",
    "## <u>Task</u>:\n",
    "### You're given an `N`-dimensional array and you would like to take the mean of that array along the\n",
    "### `n`-th (`n < N`) axis. However, you wanna remove all entries that are below a threshold `eps` and\n",
    "### not count them toward the mean.\n",
    "### \n",
    "### For instance, say that `N == 2` (matrix) and you wanna take the mean along the `n == 1` axis.\n",
    "### Since some rows may be thinned out more than others, you have to do this in a for-loop ...\n",
    "### (don't even try to understand all the details, it's futile ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "               \n",
    "               \n",
    "def mean_with_threshold(arr: np.ndarray, axis: int, eps: float) -> np.ndarray:\n",
    "  # Initialize an output array to hold the sum values and counts along the specified axis\n",
    "  shape = arr.shape[:axis] + arr.shape[axis+1:]\n",
    "  sums = np.zeros(shape, dtype=float)\n",
    "  counts = np.zeros(shape, dtype=int)\n",
    "    \n",
    "  # Iterate over all slices along the specified axis\n",
    "  for multi_index in product(*map(range, sums.shape)):\n",
    "    slice_selector = multi_index[:axis] + (slice(None),) + multi_index[axis:]\n",
    "    values = arr[slice_selector]\n",
    "\n",
    "    # Apply a boolean mask to filter values based on the threshold\n",
    "    filtered_values = values[values >= eps]\n",
    "\n",
    "    # Update the sums and counts based on the filtered values\n",
    "    if filtered_values.size > 0:\n",
    "      sums[multi_index] = np.sum(filtered_values)\n",
    "      counts[multi_index] = filtered_values.size\n",
    "    \n",
    "    # Compute the mean by dividing the sums by the counts (but only if the counts > 0)\n",
    "  return np.divide(sums, counts, where=(counts > 0))\n",
    "               \n",
    "               \n",
    "A = np.random.randn(3, 5, 10, 7, 3, 4)\n",
    "\n",
    "print(f\"Mean alsong axis 2 equals masked mean along axis 2? {np.allclose(A.mean(axis=2), mean_with_threshold(A, 2, -0.02))} (to be expected).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59b1b7",
   "metadata": {},
   "source": [
    "### \n",
    "### There is a laughably easy solution to the above.\n",
    "### The `np.ma.masked_array` allows you to pass a boolean mask `mask`\n",
    "### that is true where a value is **missing** (not where it is not missing).\n",
    "### Check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean_with_threshold(arr: np.ndarray, axis: int, eps: float) -> np.ndarray:\n",
    "  return np.ma.masked_array(arr, mask=arr < eps).mean(axis=axis).view(np.ndarray)  # convert back to normal array\n",
    "\n",
    "print(f\"Both implementations give the same outclome?  {np.allclose(masked_mean_with_threshold(A, 2, -0.02), mean_with_threshold(A, 2, -0.02))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3310e2",
   "metadata": {},
   "source": [
    "### Even though we already know the outcome, let's time the two implementations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit mean_with_threshold(A, 2, -0.02)\n",
    "%timeit masked_mean_with_threshold(A, 2, -0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ae5e2",
   "metadata": {},
   "source": [
    "### \n",
    "### Not surprising ;-)\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "# <u> What we have learned: </u>\n",
    "### 1. Not using `numpy` vectorisation is a bad idea.\n",
    "### 2. Vectorisation becomes easier once we start thinking in terms of shapes.\n",
    "### 3. We can vectorise operations on arbitrarily-dimensional arrays using `Ellipsis` `...`\n",
    "### and passing a tuple containing `slice`'s and `np.newaxis`'s:\n",
    "### `arr[ (slice(None),) * m + (_,) ] ...`\n",
    "### 4. It is worth it to learn more about `np.ufunc` arguments, like `out=arr`.\n",
    "### 5. Missing data is conveniently handled using `np.ma.masked_array`.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "\n",
    "# <u>(Short) Lesson 2</u>:\n",
    "## Writing a parallelised `Python` code.\n",
    "### \n",
    "### By default a `Python` program runs largely on just one core.\n",
    "### There is a thing called the _Global Interpreter Lock_ which prevents more than one\n",
    "### process at a time to execute `Python` bytecode. The GIL has been introduced and kept\n",
    "### for purposes of safety. It is against the philosophy of Python to allow for unsafe\n",
    "### operations. There are ongoing projects that work on a `Python` implementation that\n",
    "### allows for more daring parallelisation but, so far, none of them have really been successful.\n",
    "### The most successful project of this sort, so far, is `PyPy`. However, `PyPy` does not\n",
    "### support all `Numpy` features yet ...\n",
    "### \n",
    "### There is a way to parallelise `Python` code though. We can simply spawn several\n",
    "### `Python` interpreters, each with their own copy of the data. A downside is that the data\n",
    "### has to be copied.\n",
    "### (There is also a multi-threaded `Python` sort-of parallelisation but it is less useful in scientific computing).\n",
    "### \n",
    "### `Python` parallelisation is useful for non memory-bound tasks and since it spawns several\n",
    "### interpreters, writing a parallel `Python` code is easier and safer than in most languages.\n",
    "### \n",
    "## <u>Task </u>:\n",
    "### Given a regular grid over, say, $[0, 1]$, and a sufficiently regular function $f$,\n",
    "### write a parallelised code that integrates the function over the grid using Gaussian integration.\n",
    "### We can do this, for instance using `concurrent.futures`.\n",
    "### <u>Code</u>: (not vectorised for educational purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2663b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load parallel/quad.py\n",
    "import numpy as np\n",
    "from concurrent import futures\n",
    "from typing import Callable\n",
    "import time\n",
    "\n",
    "\n",
    "### THIS PART IS NOT SO IMPORTANT\n",
    "\n",
    "# Sequential integration\n",
    "def integrate_sequential(f, nelems, order):\n",
    "  x = np.linspace(0, 1, nelems + 1)\n",
    "  grid = np.stack([x[:-1], x[1:]], axis=1)\n",
    "  points, weights = np.polynomial.legendre.leggauss(order)\n",
    "  points = (points + 1) / 2\n",
    "  weights = weights / 2\n",
    "\n",
    "  result = 0.0\n",
    "  for a, b in grid:\n",
    "    result += (b - a) * (weights * f((b - a) * points + a)).sum()\n",
    "  return result\n",
    "\n",
    "\n",
    "# Sequential function that is used for parallel execution\n",
    "def integrate_subgrid(args):\n",
    "  f, grid, order, fargs = args  # unpack all arguments. Note that fargs is passed to f\n",
    "  points, weights = np.polynomial.legendre.leggauss(order)\n",
    "  points = (points + 1) / 2\n",
    "  weights = weights / 2\n",
    "\n",
    "  result = 0.0\n",
    "  for a, b in grid:\n",
    "    result += (b - a) * (weights * f((b - a) * points + a, *fargs)).sum()\n",
    "  return result\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "### FOCUS ON THIS PART\n",
    "\n",
    "\n",
    "# Parallel integration using ProcessPoolExecutor\n",
    "def integrate_parallel(f: Callable, fargs, nelems: int, nprocs=4, order=3):\n",
    "  \"\"\"\n",
    "    Parallel version of the sequential implementation from above.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f: positional argument only function to integrate, of the form f(x, *fargs)\n",
    "    fargs: additional positional arguments passed to f, i.e., f(x, *fargs)\n",
    "    nelems: number of elements to divide [0, 1] into\n",
    "    nprocs: maximum number of parallel processes\n",
    "    order: gaussian integration order\n",
    "  \"\"\"\n",
    "  x = np.linspace(0, 1, nelems + 1)\n",
    "  grid = np.stack([x[:-1], x[1:]], axis=1)  # [[a0, a1], [a1, a2], ...]\n",
    "\n",
    "  # Split grid into roughly equal-sized subgrids for parallel processing\n",
    "  subgrids = np.array_split(grid, nprocs, axis=0)\n",
    "\n",
    "  args = [(f, subgrid, order, fargs) for subgrid in subgrids]\n",
    "\n",
    "  with futures.ProcessPoolExecutor(max_workers=nprocs) as executor:\n",
    "    results = list(executor.map(integrate_subgrid, args))\n",
    "\n",
    "  return sum(results)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "def f(x, w):\n",
    "  return np.sin(w * x)\n",
    "\n",
    "\n",
    "def main(nelems=100000, order=3, nprocs=4):\n",
    "  w = 18 * np.pi\n",
    "\n",
    "  # keep track of computational times\n",
    "  t0 = time.time()\n",
    "  result_seq = integrate_sequential(lambda x: f(x, w=w), nelems, order)\n",
    "  t1 = time.time()\n",
    "  result_par = integrate_parallel(f, (w,), nelems, nprocs=nprocs, order=order)\n",
    "  t2 = time.time()\n",
    "\n",
    "  # Check if sequential and parallel results are equivalent\n",
    "  print(f\"Parallel and sequential integral is the same? {np.allclose(result_seq, result_par)} \\n\\n\")\n",
    "\n",
    "  print(f\"nprocs = {nprocs}, so we expect a maximum speedup by {nprocs}.\\n\\n\")\n",
    "  print(f\"The sequential implementation took {t1 - t0} seconds.\\n\\n\")\n",
    "  print(f\"The parallel implementation took {t2 - t1} seconds.\\n\\n\")\n",
    "  print(\"Parallelisation gives a speedup by a factor of \\033[4m{}\\033[0m. \\n\\n\".format((t1 - t0) / (t2 - t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef081832",
   "metadata": {},
   "source": [
    "### \n",
    "### Jupyter Notebook can't run parallel code from a cell (in Windows and MacOs) so we have to import from the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel.quad import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function from above for various grid densities.\n",
    "for power in (3, 4, 5, 6, 7):  # the last one's gonna take a while.\n",
    "  nelems = 10 ** power\n",
    "  print(f\"Running the sequential and parallel versions for {nelems} elements.\\n\\n\")\n",
    "  main(nelems=int(10 ** power))\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dd919",
   "metadata": {},
   "source": [
    "### \n",
    "### We conclude that it takes a particular problem size before parallelisation becomes worth it.\n",
    "### This is because spawning a process is itself costly.\n",
    "### \n",
    "### The biggest (coding) challenge of `Python` parallelisation is that all functions have to be \n",
    "### defined in the `global scope` of the script, i.e., the scope with zero indentation.\n",
    "### \n",
    "### Writing a code that allows you to create the function you wanna work with at runtime requires\n",
    "### extensive use of combining pre-defined functions via compositions and argument forwarding.\n",
    "### \n",
    "### It is possible but generally less readable and requires more careful planning.\n",
    "### Use it with care and make sure it is actually worth the extra work.\n",
    "### \n",
    "## <u>Exercise 2.1</u>:\n",
    "### Vectorise above code\n",
    "### \n",
    "### <u>Template</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_ = np.newaxis\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def integrate_vectorised(f, nelems, order):\n",
    "  x = np.linspace(0, 1, nelems + 1)\n",
    "  points, weights = np.polynomial.legendre.leggauss(order)\n",
    "  points = (points + 1) / 2\n",
    "  weights = weights / 2\n",
    "  \n",
    "  a, b = x[:-1], x[1:]\n",
    "\n",
    "  return # YOUR ONE-LINER HERE\n",
    "\n",
    "\n",
    "f = lambda x: np.sin(18 * np.pi * x)\n",
    "\n",
    "# Run the vectorised function for several input element sizes\n",
    "for power in (3, 4, 5, 6, 7):\n",
    "  nelems = 10 ** power\n",
    "  t0 = time.time()\n",
    "  integrate_vectorised(f, nelems, 3)\n",
    "  t1 = time.time()\n",
    "  print(f\"The vectorised operation with {nelems} elements took {t1 - t0} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7047fb",
   "metadata": {},
   "source": [
    "### \n",
    "### <u>solution</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adaa73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_ = np.newaxis\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def integrate_vectorised(f, nelems, order):\n",
    "  x = np.linspace(0, 1, nelems + 1)\n",
    "  points, weights = np.polynomial.legendre.leggauss(order)\n",
    "  points = (points + 1) / 2\n",
    "  weights = weights / 2\n",
    "  \n",
    "  a, b = x[:-1], x[1:]\n",
    "\n",
    "  return ((b - a)[:, _] * weights * f( (b - a)[:, _] * points + a[:, _] )).sum()\n",
    "\n",
    "\n",
    "f = lambda x: np.sin(18 * np.pi * x)\n",
    "\n",
    "# Run the vectorised function for several input element sizes\n",
    "for power in (3, 4, 5, 6, 7):\n",
    "  nelems = 10 ** power\n",
    "  t0 = time.time()\n",
    "  integrate_vectorised(f, nelems, 3)\n",
    "  t1 = time.time()\n",
    "  print(f\"The vectorised operation with {nelems} elements took {t1 - t0} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490d000",
   "metadata": {},
   "source": [
    "### \n",
    "### To underline again the importance of vectorisation ...\n",
    "### \n",
    "### Of course, a vectorised code like the one from exercise 2.1 can also be paralellised.\n",
    "### In that case, we would split the vectorised operations into batches.\n",
    "### \n",
    "### There is a library that simplifies this, called `Dask`. Check it out in case you're interested.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "# <u>Lesson 3</u>:\n",
    "## Using `Numba` for `JIT compilation`.\n",
    "### \n",
    "### We have seen how we can use `Numpy` vectorisation and `Python` parallelisation for considerable\n",
    "### speedups. However, sometimes it's just not good enough. For this, since $2012$, there has been \n",
    "### a `JIT` (just in time) compiler called `Numba`.\n",
    "### \n",
    "### The strengths of `Numba` are:\n",
    "### 1. You write `Numba` functions directly in your `Python` script, no separate `Numba` scripts needed.\n",
    "### 2. You write a normal `Python` function and decorate it as a `Numba` function, that's it.\n",
    "### You don't really need to learn anything new for it.\n",
    "### 3. It compiles to blazing fast `C` code, directly accessible from your `Python` implementation.\n",
    "### 4. It supports `embarassing parallelisation` using `prange` instead of `range`.\n",
    "### \n",
    "### The weaknesses:\n",
    "### 1. Not all valid `Python` code is also valid `Numba` code.\n",
    "### 2. If you want to write a complex `Python` program in pure `Numba` you'll soon run into its limitations.\n",
    "### 3. It's much trickier to generalise operations to arbitrary dimension, especially when for-loops are needed.\n",
    "\n",
    "### \n",
    "### `Numba` is best for writing relatively small functions that perform an operation that cannot be\n",
    "### vectorised or for which vectorisation runs into memory issues.\n",
    "### \n",
    "### We come back to our `Numpy` implementation of the mesh union code from the first Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fa533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# First some utility functions\n",
    "\n",
    "\n",
    "def plot_meshes(list_of_elements, list_of_points):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for elems, points in zip(list_of_elements, list_of_points):\n",
    "        ax.triplot(*points.T, elems, alpha=0.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_mesh(nx: int, ny: int, translate=None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "  \"\"\"\n",
    "    Create a regular triangular mesh over [0, 1] x [0, 1].\n",
    "    Can optionally be translated by a vector `translate`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nx: number of mesh vertices in x-direction\n",
    "    ny: number of mesh vertices in y-direction\n",
    "    translate: (optional) translation vector.\n",
    "  \"\"\"\n",
    "  \n",
    "  points = np.stack(list(map(np.ravel, np.meshgrid(np.linspace(0, 1, nx),\n",
    "                                                   np.linspace(0, 1, ny) ))), axis=1)    \n",
    "  \n",
    "  if translate is not None:\n",
    "    points += np.asarray(translate)[None]\n",
    "    \n",
    "  points = np.round(points, 8)\n",
    "\n",
    "  \n",
    "  indices = (np.arange(nx * ny).reshape(ny, nx)[:-1, :-1]).ravel()\n",
    "  quads = np.stack([indices, indices+1, indices+ny+1, indices+ny], axis=1)\n",
    "  elements = quads[:, [0, 1, 2, 0, 2, 3]].reshape(-1, 3)\n",
    "  return elements, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d85188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_mesh_union_numpy(elems0, points0, elems1, points1):\n",
    "  \"\"\"\n",
    "    Take the unions of two meshes using only Python and Numpy\n",
    "    functionality.\n",
    "  \"\"\"\n",
    "  # get all unique points of the two sets of points\n",
    "  new_points = np.unique(np.concatenate([points0, points1]), axis=0)\n",
    "  \n",
    "  \n",
    "  ### These two operations require a pure Python for loop and cannot\n",
    "  ### be vectorised.\n",
    "\n",
    "  # map each unique point to an index\n",
    "  map_point_index = dict(zip(map(tuple, new_points), count()))\n",
    "\n",
    "  # map both meshes' elements' points to the new index\n",
    "  mapped_elems = np.apply_along_axis(lambda x: map_point_index[tuple(x)],\n",
    "                                     axis=-1,\n",
    "                                     arr=np.concatenate([points0[elems0], points1[elems1]]))\n",
    "  \n",
    "  ###\n",
    "  ###\n",
    "\n",
    "  # find the indices of the first occurence of the transformed elements that are unique\n",
    "  _, unique_indices = np.unique(np.sort(mapped_elems, axis=1), return_index=True, axis=0)\n",
    "\n",
    "  # keep only the unique occurences\n",
    "  new_elems = mapped_elems[unique_indices]\n",
    "\n",
    "  return new_elems, new_points\n",
    "\n",
    "\n",
    "elems0, points0 = create_mesh(51, 51)\n",
    "elems1, points1 = create_mesh(51, 51, translate=[0.5, 0])\n",
    "\n",
    "plot_meshes([elems0, elems1], [points0, points1])\n",
    "\n",
    "elems_np, points_np = take_mesh_union_numpy(elems0, points0, elems1, points1)\n",
    "plot_meshes([elems_np], [points_np])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1619d",
   "metadata": {},
   "source": [
    "### \n",
    "### The `Numpy` implementation is obviously bottlenecked by the operation\n",
    "### that involves the `hashmap`. \n",
    "### \n",
    "### We may outsource this step to a `Numba` routine.\n",
    "### Check this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def make_numba_indexmap(points):\n",
    "  # The function is compiled for the input type the first time it is called with that input\n",
    "  \"\"\"\n",
    "    Create a hashmap that maps each point in `points` to a unique running index.\n",
    "    Assumes the points in `points` to already be unique.\n",
    "    \n",
    "    This initialisation of the hashmap is not pythonic but we have to do it in\n",
    "    this way because numba does not support the ingredients for the pythonic\n",
    "    implementation.\n",
    "  \"\"\"\n",
    "  map_coord_index = {}\n",
    "  i = 0\n",
    "  for point in points:\n",
    "    # We cannot convert array to tuple directly because the length\n",
    "    # of the tuple has to be known at compile time.\n",
    "    map_coord_index[ (point[0], point[1]) ] = i\n",
    "    i += 1\n",
    "\n",
    "  return map_coord_index\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def renumber_elements_from_indexmap(elements, points, map_coord_index):\n",
    "  \"\"\"\n",
    "    Given a 2D array whose rows represent element,\n",
    "    a 2D array of points and a map that maps each point in \n",
    "    `points` to a unique index, renumber the elements using the\n",
    "    indexmap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    elements: element array to renumber\n",
    "    points: 2D element vertex coordinates\n",
    "    map_cord_index: hashmap mapping a tuple of coordinates to a\n",
    "                    unique index.\n",
    "  \"\"\"\n",
    "  newelems = np.empty(elements.shape, dtype=np.int64)\n",
    "  \n",
    "  for i in range(len(elements)):  # run the assignment in parallel\n",
    "    \n",
    "    myvertices = elements[i]\n",
    "    for j, point in enumerate(points[myvertices]):\n",
    "      newelems[i, j] = map_coord_index[ (point[0], point[1]) ]\n",
    "      \n",
    "  return newelems\n",
    "\n",
    "\n",
    "def take_mesh_union_numba(elems0, points0, elems1, points1):\n",
    "  \"\"\"\n",
    "    Take the union of two meshes using Numpy and Numba functionality.\n",
    "  \"\"\"\n",
    "  new_points = np.unique(np.concatenate([points0, points1]), axis=0)\n",
    "  \n",
    "  \n",
    "  ### Now we use the new Numba implementation\n",
    "\n",
    "  # map each unique point to an index in Numba\n",
    "  map_point_index = make_numba_indexmap(new_points)\n",
    "    \n",
    "  mapped_elems = \\\n",
    "    np.concatenate([ \n",
    "                      renumber_elements_from_indexmap(myelems, mypoints, map_point_index)\n",
    "                      for myelems, mypoints in zip([elems0, elems1], [points0, points1]) \n",
    "                   ])\n",
    "  \n",
    "  ### The rest is the same\n",
    "  _, unique_indices = np.unique(np.sort(mapped_elems, axis=1), return_index=True, axis=0)\n",
    "  new_elems = mapped_elems[unique_indices]\n",
    "\n",
    "  return new_elems, new_points\n",
    "\n",
    "\n",
    "elems0, points0 = create_mesh(51, 51)\n",
    "elems1, points1 = create_mesh(51, 51, translate=[0.5, 0])\n",
    "\n",
    "elems_nb, points_nb = take_mesh_union_numba(elems0, points0, elems1, points1)\n",
    "\n",
    "print(f\"Numpy and numba produce the same mesh union ? {(elems_nb == elems_np).all()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350c34d",
   "metadata": {},
   "source": [
    "### \n",
    "### We make a big mesh and time both implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b034adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "elems0, points0 = create_mesh(101, 101)\n",
    "elems1, points1 = create_mesh(101, 101, translate=[0.5, 0])\n",
    "\n",
    "\n",
    "%timeit take_mesh_union_numpy(elems0, points0, elems1, points1)\n",
    "%timeit take_mesh_union_numba(elems0, points0, elems1, points1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f988a",
   "metadata": {},
   "source": [
    "### \n",
    "### Quite an impressive speedup just by jitting away the hashmap !\n",
    "### \n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### `Numba` is perfect for taking a pure `Python / Numpy` code, identifying\n",
    "### performance bottlenecks and jitting them away. It's important to write\n",
    "### a `modular` code consisting of many puzzle pieces.\n",
    "### \n",
    "### Do not expect any performance gains from jitting away single `Numpy`\n",
    "### functions. In fact, `Numpy` is so well optimised that the `Numba`\n",
    "### equivalent tends to be a tad slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "sin_numpy = np.sin\n",
    "\n",
    "\n",
    "@njit(cache=True, fastmath=True)\n",
    "def sin_numba(x):\n",
    "  return np.sin(x)\n",
    "\n",
    "\n",
    "x = np.linspace(0, 1, 1001)\n",
    "\n",
    "sin_numba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224155c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sin_numpy(x)\n",
    "%timeit sin_numba(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91b154",
   "metadata": {},
   "source": [
    "### \n",
    "### Performance gains can be expected when `Numpy` vectorisation is not\n",
    "### (100%) possible. \n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### \n",
    "### <u>Task </u>:\n",
    "### You have an `np.arange` of `N` increasing indices representing degrees\n",
    "### of freedom in a FEM basis (or whatever). \n",
    "### You are given an integer array of pairs that you're coupling (each fusing 2 DOFs in one).\n",
    "### Modify the DOF array inplace to always point to the lower of the two DOFs you're coupling.\n",
    "### Then renumber the dofs in ascending order.\n",
    "### \n",
    "### Example:\n",
    "### `N == 5`, `dofs = [0, 1, 2, 3, 4]`, `pairs = [ [1, 2], [0, 1], [3, 4] ]`\n",
    "### couple: `dofs = [0, 0, 0, 3, 3]`\n",
    "### renumber: `dofs = [0, 0, 0, 1, 1]`\n",
    "### \n",
    "### One implementation in pure `Numpy` and one using `Numba`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# I don't know how to vectorise this one\n",
    "def _apply_pairs(dofs: np.ndarray, pairs: np.ndarray) -> np.ndarray:\n",
    "  for pair in pairs:\n",
    "    minindex = np.min(dofs[pair])\n",
    "    dofs[pair] = minindex\n",
    "  return dofs\n",
    "\n",
    "\n",
    "def renumber_np(ndofs: int, pairs: np.ndarray) -> np.ndarray:\n",
    "  dofs = np.arange(ndofs)\n",
    "  pairs = pairs[ np.lexsort(pairs.T[::-1]) ]  # lexsort pairs in place\n",
    "  \n",
    "  dofs = _apply_pairs(dofs, pairs)\n",
    "  \n",
    "  return np.unique(dofs, return_inverse=True)[1]\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def _apply_pairs_nb(dofs, pairs):\n",
    "  for pair in pairs:\n",
    "    minindex = np.min(dofs[pair])\n",
    "    dofs[pair] = minindex\n",
    "  return dofs\n",
    "\n",
    "\n",
    "def renumber_nb(ndofs: int, pairs: np.ndarray) -> np.ndarray:\n",
    "  dofs = np.arange(ndofs)\n",
    "  pairs = pairs[ np.lexsort(pairs.T[::-1]) ]  # lexsort pairs in place\n",
    "  \n",
    "  dofs = _apply_pairs_nb(dofs, pairs)\n",
    "  \n",
    "  return np.unique(dofs, return_inverse=True)[1]\n",
    "\n",
    "\n",
    "\n",
    "# First a sanity check:\n",
    "pairs = np.array([[1, 2], [0, 1], [3, 4]])\n",
    "print(f\"Passing the data from the example gives: {renumber_np(5, pairs)}.\")\n",
    "\n",
    "\n",
    "ndofs = 1_000_000\n",
    "npairs = 200_000\n",
    "\n",
    "# create some random pairs\n",
    "pairs = np.unique(np.random.randint(0, ndofs, (npairs, 2)), axis=1)  \n",
    "pairs = pairs[ pairs[:, 0] != pairs[:, 1] ]\n",
    "\n",
    "print(f\"Numpy and numba give the same outcomes? {(renumber_np(ndofs, pairs) == renumber_nb(ndofs, pairs)).all()}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88832f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit renumber_np(ndofs, pairs)\n",
    "%timeit renumber_nb(ndofs, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988daa1",
   "metadata": {},
   "source": [
    "### \n",
    "### A `7x` speedup (on my machine) just by jitting the `_apply_pairs` function.\n",
    "### \n",
    "### Another application of `Numba` is jitting `Numpy` vectorisable functions \n",
    "### that would run into memory issues for big inputs. We can also often parallelise\n",
    "### such operations using `embarassing parallelisation`.\n",
    "### \n",
    "### Let's check out again our vectorised implementation of the integrate function\n",
    "### from lesson one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load numba/integrate.py\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "import time\n",
    "\n",
    "_ = np.newaxis\n",
    "\n",
    "\n",
    "@njit(cache=True, parallel=True, fastmath=True)\n",
    "def _integrate_numba_parallel(f, nelems, weights, points):\n",
    "  \"\"\"\n",
    "    We have to pass weights and points because\n",
    "    np.polynomial.legendre.leggaus is not available\n",
    "    inside of a Numba function.\n",
    "    We can however pass Numba jitted functions `f` as arguments.\n",
    "  \"\"\"\n",
    "  ret = np.empty((4,), dtype=np.float64)\n",
    "  batch_size = nelems // 4\n",
    "  for i in prange(4):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size if i != 3 else nelems\n",
    "\n",
    "    val = 0.0\n",
    "    for j in range(start, end):\n",
    "      a, b = j / nelems, (j+1) / nelems\n",
    "\n",
    "      for k in range(len(weights)):\n",
    "        val += (b - a) * weights[k] * f((b - a) * points[k] + a)\n",
    "\n",
    "    ret[i] = val\n",
    "\n",
    "  return ret.sum()\n",
    "\n",
    "\n",
    "@njit(cache=True, fastmath=True)\n",
    "def _integrate_numba_sequential(f, nelems, weights, points):\n",
    "  \"\"\"\n",
    "    We have to pass weights and points because\n",
    "    np.polynomial.legendre.leggaus is not available\n",
    "    inside of a Numba function.\n",
    "    We can however pass Numba jitted functions `f` as arguments.\n",
    "  \"\"\"\n",
    "  val = 0.0\n",
    "  for j in range(0, nelems):\n",
    "    a, b = j / nelems, (j+1) / nelems\n",
    "\n",
    "    for k in range(len(weights)):\n",
    "      val += (b - a) * weights[k] * f((b - a) * points[k] + a)\n",
    "\n",
    "  return val\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def _sin(x):\n",
    " return np.sin(18 * np.pi * x)\n",
    "\n",
    "\n",
    "def integrate_numba(f, nelems, order, parallel=True):\n",
    "  points, weights = np.polynomial.legendre.leggauss(order)\n",
    "  points = (points + 1) / 2\n",
    "  weights = weights / 2\n",
    "  return {True: _integrate_numba_parallel,\n",
    "          False: _integrate_numba_sequential}[parallel](f, nelems, weights, points)\n",
    "\n",
    "\n",
    "integrate_numba(_sin, 10, 3, parallel=False)\n",
    "\n",
    "# Run the numba function for several input element sizes sequentially\n",
    "for power in (2, 3, 5, 6, 7, 8):\n",
    "  nelems = 10 ** power\n",
    "  t0 = time.time()\n",
    "  integrate_numba(_sin, nelems, 3, parallel=False)\n",
    "  t1 = time.time()\n",
    "  print(f\"The sequential Numba implementation with {nelems} elements took {t1 - t0} seconds.\\n\")\n",
    "\n",
    "\n",
    "integrate_numba(_sin, 10, 3, parallel=True)\n",
    "\n",
    "# Run the numba function for several input element sizes in parallel\n",
    "for power in (2, 3, 5, 6, 7, 8):\n",
    "  nelems = 10 ** power\n",
    "  t0 = time.time()\n",
    "  integrate_numba(_sin, nelems, 3, parallel=True)\n",
    "  t1 = time.time()\n",
    "  print(f\"The parallel Numba implementation with {nelems} elements took {t1 - t0} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4cf589",
   "metadata": {},
   "source": [
    "### \n",
    "### A little code from my PhD (I was inexperienced compared to now, so it's not the best code).\n",
    "### \n",
    "<img src=\"img/jitted_phd_code.png\" alt=\"Drawing\" width=\"700\"/>\n",
    "\n",
    "### \n",
    "### The entire scipt is $1600$ lines of code. Jitting just these two functions\n",
    "### (and two more small auxiliary functions) sped up my code by a factor of $12$ !\n",
    "### \n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21453a9",
   "metadata": {},
   "source": [
    "# <u> What we have learned: </u>\n",
    "### 1. Writing `Numba` functions is relatively easy if we already know `Python`.\n",
    "### 2. Identifying performance bottlenecks and writing small jitted functions can be a highly effective strategy.\n",
    "### 3. While gaining performance, we give up some flexibility, `tuple(point) -> (point[0], point[1])`.\n",
    "### \n",
    "### If you do things related to optimisation, I can recommend `JAX`.\n",
    "### It's similar to `Numba` but can also differentiate your code for computing gradients.\n",
    "### \n",
    "<hr style=\"border:1px solid blue\">\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625254d5",
   "metadata": {},
   "source": [
    "# <u>Lesson 4</u>:\n",
    "## Pimping your code with `Cython`.\n",
    "### \n",
    "### `Numba` is dope but my experience is that at some point, it will simply not be enough.\n",
    "### \n",
    "### In case you need better performance but the restrictions of `Numba` (such as \n",
    "### rudimentary support for classes, structs and enums) prevent you from\n",
    "### really getting the job done, the next thing you should try is `Cython`.\n",
    "### \n",
    "### The standard `Python` interpreter is written in `C`. What `Cython` does is, it \n",
    "### has you write code in a `Python`-ish language and translates it to a `C` file (you can actually see it).\n",
    "### Then, in the `C` program, for all things that are known at compile time, it writes a `C` function\n",
    "### and for the remaining stuff, it invokes the `Python` interpreter from the `C` file.\n",
    "### \n",
    "### Here are some of `Cython`'s strengths:\n",
    "### 1. 99.9% of valid `Python` code is also valid `Cython` code.\n",
    "### 2. You don't have to write everything in `C` style. Some parts of your code you want to and can keep interpreted.\n",
    "### 3. Calling the compiled `Cython` functionality from pure `Python` is a piece of cake.\n",
    "### 4. It supports classes, structs but also things like enums.\n",
    "### \n",
    "### Here some weaknesses:\n",
    "### 1. `Cython` is `ahead of time` rather than `JIT` compiled.\n",
    "### 2. It requires learning a new, albeit very similar, language to unlock all performance features.\n",
    "### 3. If you really want a super performant and parallelised code, you're closer to the rather\n",
    "### unsafe `C` coding style than in `Numba` and you have to give up more `Numpy` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45d95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52a53f",
   "metadata": {},
   "source": [
    "### \n",
    "### `Cython` allows you to staticly type variables, significantly reducing the \n",
    "### `Python` type checking overhead. This can make a tremendous difference in\n",
    "### performance. Let's assume we're bad at numpy vectorisation and just wanna\n",
    "### create a list of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def squares_py(nsquares: int) -> List[int]:\n",
    "  return [i**2 for i in range(nsquares)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea79c5",
   "metadata": {},
   "source": [
    "### We can use `Cython` to properly type declare all variables used within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722211af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "    \n",
    "\n",
    "# cdef = can only be used from a cython environment\n",
    "# cpdef = can be used both from cython and python\n",
    "# def  = only python\n",
    "\n",
    "cpdef list[int] squares_cy(int nsquares):\n",
    "  # this block allows you to type variables\n",
    "  # used within the function.\n",
    "  cdef:\n",
    "    int i\n",
    "  return [i**2 for i in range(nsquares)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97da8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit squares_py(50000)\n",
    "%timeit squares_cy(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d6939",
   "metadata": {},
   "source": [
    "### \n",
    "### Already quite a performance gain from a simple type declaration at the\n",
    "### beginning. \n",
    "### `Cython` still treats the `list` `ret` as a `PyObject`, thus invoking the\n",
    "### interpreter. However, by typing it as a `PyObject` `list` and `i` as an `int`,\n",
    "### we were already able to accomplish some impressive speedups.\n",
    "### \n",
    "### `Cython` has the advantage over `Numba` that you can pass and type declare\n",
    "### all `Python` built-ins and even custom objects in a `Cython` function.\n",
    "### \n",
    "### However, does `Cython` speed up non-vectorised `Numpy` code ?\n",
    "### Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_not_vectorised(arr: np.ndarray) -> np.ndarray:\n",
    "  \"Not vectorised.\"\n",
    "  assert arr.ndim == 2\n",
    "  ret = np.empty(arr.shape, dtype=float)\n",
    "  for i, subarr in enumerate(arr):\n",
    "    ret[i] = subarr / np.linalg.norm(subarr)\n",
    "  return ret\n",
    "\n",
    "\n",
    "def normalize_vectorised(arr: np.ndarray) -> np.ndarray:\n",
    "  \"vectorised.\"\n",
    "  return arr / np.linalg.norm(arr, ord=2, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport numpy as cnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cpdef cnp.ndarray[cnp.float64_t, ndim=2] normalize_not_vectorised_cy(cnp.ndarray[cnp.float64_t, ndim=2] arr):\n",
    "  cdef:\n",
    "    int i\n",
    "    cnp.ndarray[cnp.float64_t, ndim=1] subarr\n",
    "    cnp.ndarray[cnp.float64_t, ndim=2] ret\n",
    "  \n",
    "  ret = np.empty((arr.shape[0], arr.shape[1]), dtype=float)  # we have to do (arr.shape[0], arr.shape[1]) instead of arr.shape\n",
    "  \n",
    "  for i, subarr in enumerate(arr):\n",
    "    ret[i] = subarr / np.linalg.norm(subarr)\n",
    "  \n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randn(10_000, 2)\n",
    "\n",
    "%timeit normalize_not_vectorised(arr)\n",
    "%timeit normalize_vectorised(arr)\n",
    "%timeit normalize_not_vectorised_cy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be593c",
   "metadata": {},
   "source": [
    "### \n",
    "### The reason we didn't gain any speedups is because `Cython`, unlike `Numba`, still does\n",
    "### `Numpy` stuff from the `Python` interpreter. Here our code is bottlenecked by the `for` loop.\n",
    "### \n",
    "### We can pass `Numpy` arrays into `Cython` functions and `view` them as a so-called `Memoryview`.\n",
    "### A `Memoryview` is a view into the memory that is aware of its own shape.\n",
    "### Let us remove all `Python` overhead and see if we gain speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "\n",
    "# double[:, ::1] means 2D double array contiguous in memory \n",
    "cpdef double[:, ::1] normalize_cy(double[:, ::1] arr):\n",
    "  cdef:\n",
    "    int i, j\n",
    "    double norm\n",
    "    \n",
    "  cdef double[:, ::1] ret = np.empty((arr.shape[0], arr.shape[1]), dtype=np.double)\n",
    "  \n",
    "  # C style implementation\n",
    "  for i in range(arr.shape[0]):\n",
    "    norm = 0.0\n",
    "    for j in range(arr.shape[1]):\n",
    "      norm += arr[i, j] ** 2\n",
    "    norm = sqrt(norm)\n",
    "    \n",
    "    for j in range(arr.shape[1]):\n",
    "      ret[i, j] = arr[i, j] / norm\n",
    "      \n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randn(10_000, 2)\n",
    "\n",
    "%timeit normalize_cy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3ff05",
   "metadata": {},
   "source": [
    "### \n",
    "### Even faster than the vectorised version because we didn't create an\n",
    "### intermediate array for the norm.\n",
    "### The only `Python` overhead left is the line:\n",
    "### `cdef double[:, ::1] ret = np.empty((arr.shape[0], arr.shape[1]), dtype=np.double)`\n",
    "### which creates a _smart_ array that is automatically deallocated (because it's handled by `Numpy`).\n",
    "### \n",
    "### We can speed up more by disabling bounds and division by zero checking, trading safety for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83acfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "import numpy as np\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.cdivision(True)\n",
    "cpdef double[:, ::1] normalize_cy(double[:, ::1] arr):\n",
    "  cdef:\n",
    "    int i, j\n",
    "    double norm\n",
    "    \n",
    "  cdef double[:, ::1] ret = np.empty((arr.shape[0], arr.shape[1]), dtype=np.double)\n",
    "  \n",
    "  # C style implementation\n",
    "  for i in range(arr.shape[0]):\n",
    "    norm = 0.0\n",
    "    for j in range(arr.shape[1]):\n",
    "      norm += arr[i, j] ** 2\n",
    "    norm = sqrt(norm)\n",
    "    \n",
    "    for j in range(arr.shape[1]):\n",
    "      ret[i, j] = arr[i, j] / norm\n",
    "      \n",
    "  return np.asarray(ret)  # convert back to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7321ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.4 µs ± 2.07 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.randn(10_000, 2)\n",
    "\n",
    "%timeit normalize_cy(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d8418",
   "metadata": {},
   "source": [
    "### \n",
    "### It's not much but many pennies make a dollar ;-)\n",
    "### \n",
    "### Just like `Numba`, `Cython` comes with `embarassing parallelisation` out of the box.\n",
    "### For this, we have to locally release the `GIL` (we have to import from external module\n",
    "### else the parallelisation doesn't work).\n",
    "### <u>Code</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load cy/normalize.pyx\n",
    "cimport cython\n",
    "from libc.math cimport sqrt\n",
    "from cython.parallel cimport prange\n",
    "cimport numpy as cnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.cdivision(True)\n",
    "cdef double[:, ::1] _normalize_cy_parallel(double[:, ::1] arr):\n",
    "  cdef:\n",
    "    int i, j, shape0, shape1\n",
    "    double norm\n",
    "    double[:, ::1] ret\n",
    "    \n",
    "  ret = np.empty((arr.shape[0], arr.shape[1]), dtype=np.double)\n",
    "  shape0, shape1 = arr.shape[0], arr.shape[1]\n",
    "  batch_size = shape0 // 4\n",
    "  \n",
    "  with nogil:\n",
    "    # embarassingly parallelise the outer loop\n",
    "    for i in prange(shape0):\n",
    "      norm = 0.0\n",
    "      for j in range(shape1):\n",
    "        norm += arr[i, j] ** 2\n",
    "      norm = sqrt(norm)\n",
    "\n",
    "      for j in range(shape1):\n",
    "        ret[i, j] = arr[i, j] / norm\n",
    "      \n",
    "  return ret\n",
    "\n",
    "\n",
    "def normalize_cy_parallel(double[:, ::1] arr):\n",
    "  return np.asarray(_normalize_cy_parallel(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96241d3",
   "metadata": {},
   "source": [
    "### \n",
    "### Let's time the parallel version vs the sequential one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a49b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.6 ms ± 2.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "40.9 ms ± 635 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.randn(1_000_000, 20)\n",
    "import sys\n",
    "sys.path.append('cy')\n",
    "from normalize import normalize_cy_parallel\n",
    "\n",
    "%timeit normalize_cy(arr)\n",
    "%timeit normalize_cy_parallel(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d2660",
   "metadata": {},
   "source": [
    "### Let's check if they give the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e965acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parallel and sequential version give the same output ? True\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.randn(1000, 20)\n",
    "ret0 = normalize_cy(arr)\n",
    "ret1 = normalize_cy_parallel(arr)\n",
    "print(f\"The parallel and sequential version give the same output ? {np.allclose(ret0, ret1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
